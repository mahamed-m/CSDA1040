---
title: "Text Mining Airline Tweets"
author: 'Group 2: Leesann Sutherland, Maryan Mahamed, Stanislav Taov'
date: "16/02/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(textclean)
library(twitteR)
library(tm)
library(stopwords)
library(wordcloud)
library(topicmodels)
library(ggplot2)
library(data.table)
library(sentimentr)
library(dplyr)
library(tidyverse)
library(ggthemes)
library(ggcorrplot)
library(Hmisc)
library(VIM)
library(stringr)
library(tidytext)
library(textdata)
library(tidyr)
library(broom)
library(reshape2)
library(gsubfn)
library(htmlTable)
library(textmineR)

df <- read.csv('Tweets.csv', na.strings=c("", "NA"))

```

# Introduction

Companies, for a long time, have relied of consumer feedback to help improve the quality of their product or service, and today consumers are presented with an abundance of avenues to provide that feedback, from yelp, to google +, to twitter and other social media platforms. This has led to the overwhelming accumulation of data, both structured and unstructured, across all industries globally. While structured data has provided the essential statistical insights to help companies improve their decision-making process, most data collected is unstructured in the form of text, video and audio. The challenge of unlocking valuable insights from these unstructured has proven to be pivotal to the success and continued growth of businesses as thy strive to meet consumer demands. 

In this project, we explore the use of Text Mining as a means of extracting valuable, quantifiable business insights from consumer tweets. The data used, obtained from Kaggle, focuses on tweets scarped from Twitter in February 2015 concerning six major U.S. airlines. Through various data exploration techniques, we reveal some insights from the results of the previous sentiment analysis that led to the dataset used. It will be shown that the previous study sought to analyse negative sentiments, but do not explicitly reveal anything about the positives. 

Our analysis seeks to dig deeper, analysing the tweets from scratch to not only determine the sentiments, but also the major topics for each. We use the Latent Dirichlet Allocation natural language topic modeller to extract meaning from the tweets, provide insights into the consumer experience, and discover what the companies are doing right and wrong.  

The insights will be presented in a shiny app that will allow these companies to better gauge the sentiments of their customer-base, discover the areas of their serve that need improvement and assist them in their decision making process. 



# Data Preparation and Cleaning

The data used contained 15 variables with 14640 observations. The variables included the tweet_id number, airline_sentiment - a three level factor of Negative, Neutral and Positive, airline_seniment_confidence indicating the level of confidence in the sentiment categorization, negativereason - the dominant reason for the negative comment, negative_reason_confidence, airline, airline_sentiment_gold, name - the Twitter user, negativereason_gold, retweet_count, text - the actual tweet, tweet_coord, tweet_created, tweet_location, and  user_timezone. The details can be seen below:

```{r, echo=FALSE}

#as_tibble(df)
glimpse(df)

```


For the purposes of exploration goals, we removed the confidence columns, airline_sentiment_gold, negativereason_gold, user_ timezone, tweet_created, and location related columns, as they will not be used for our further analysis, and explored the 7 remaining columns for missing values. Of the remaining columns, it was discovered that there were 5462 missing values, all located in the negativereason column. 
 

```{r, echo=FALSE} 
df <- subset(df, select = -c(airline_sentiment_confidence, negativereason_confidence, airline_sentiment_gold, negativereason_gold, tweet_created, user_timezone, tweet_coord, tweet_location))

missing_values <- aggr(df, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df), cex.axis=0.5, gap=2, ylab=c("Histogram of missing data","Pattern"))

#sum(is.na(df))
```


To discover why there were NA values present in the "negative reasons" column, the airline sentiment, negative reason and airline columns were isolated, and filtered for NA values. It was discovered that NA values only existed in the negative reasons column where the tweet sentiments were labeled as either positive or neutral, amounting to 5462 NA's. The NA values in this column were treated by replacing "NA" with an empty character string. 

```{r, echo=FALSE}
#Exploring why the negative comment reason have NA
NA_Reasons <- df %>%
  select(airline_sentiment, negativereason, airline) %>%
  filter(is.na(negativereason))

#print(head(NA_Reasons))

summary(NA_Reasons)
# NA given to positive and neutral reviews
```


```{r, echo=FALSE}
#Replacing NA with blank spaces
na_vals1 <- which(is.na(df$negativereason)) #Rows where NAs are located
df$negativereason <- as.character(df$negativereason) #first convert to character strings
df[na_vals1,]$negativereason <- ""
df$negativereason <- as.factor(df$negativereason)  #convert back to factors

#summary(df$negativereason)
#Clean!
```

The details of the final dataframe used for data exploration are shown below.

```{r, echo=FALSE}

glimpse(df)

```





# Data Exploration

In order to get an idea of the customer experience between the six airline companies, we explored the distribution of airline sentiments for each company. The graph below shows that negative sentiments far outweighs the positive or neutral sentiments in the cases of American Airlines, United and US Airways, while Virgin America performs the best. However, none of the airlines appear to be performing satisfactorily, which begs the question, what are they doing wrong and what are they doing right? 

```{r, echo=FALSE}
###  Looking at the count of airline sentiments per airline  ###
sentiments <- df %>%
  select(airline, airline_sentiment) %>%
  group_by(airline, airline_sentiment) %>%
  summarise(freq = n())

ggplot(sentiments, aes(x = airline, y = freq, fill = airline_sentiment)) + 
  geom_bar(stat="identity", position = position_dodge()) + 
  theme_set(theme_bw()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Distribution of Airline Sentiments", x = 'Airline', y = 'Frequency')

```


### Negative Sentiments

There were no columns representing the positive or neutral reasons, further enforced by the presence of NA values found during the cleaning process, showing that previous work sought merely to explore the negative sentiments. We, therefore, first explored the reasons for the negative sentiments as previously determined. The data breaks down negative reasons into core categories as shown in the graph below. Apart from Delta, where the main complaint concerned flight tardiness, each of the other airline companies underperformed in customer service, with the worst airlines being American, United and US Airways. 

```{r, echo=FALSE}
###  Looking at the distribution of negative comments  ###
neg_reasons <- df %>%
  filter(airline_sentiment == "negative") %>%
  select(airline, negativereason) %>%
  group_by(airline, negativereason) %>%
  summarise(freq = n())

ggplot(neg_reasons, aes(x = negativereason, y = freq)) + 
  geom_bar(stat="identity", fill="blue") + 
  theme_set(theme_bw()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Reasons for the Negative Sentiments", x = 'Negative Reasons', y = 'Frequency') +
  facet_wrap(~ airline)

```


These categories for negative sentiments can be considered quite general, therefore, to get a better idea of what words were used to express dissatifaction a word cloud was created. To do this, tokens were unnested from the text (tweets) column, and stop words were removed. A count of the negative words was calculated, the results of which are shown below. 

```{r, echo=FALSE}

review <- tidytext::unnest_tokens(read.csv('Tweets.csv', stringsAsFactors = FALSE), word, text)
data(stop_words)

review <- review %>%#remove stop words
  anti_join(stop_words)

review %>%
  count(word, sort=TRUE)

###counting negative sentiments
negative <- get_sentiments('bing') %>%
  filter(sentiment == "negative")

neg_reviews <- review %>%
  inner_join(negative) %>%
  count(word, sort=TRUE)

#head(neg_reviews, 10)

pal <- brewer.pal(10, "Paired")

neg_reviews <- review %>%
  inner_join(negative) %>%
  count(word) %>%
  with(wordcloud(word, n,
                 rot.per = .15,
                 scale=c(5, .3),
                 max.words = 50,
                 random.order = F,
                 random.color = F,
                 colors = pal))


```

The most frequently occuring words clearly refer to delayed flights. To see which airlines had the worst reputation with respect to tardiness, all words related to "delay" were converting to "delayed", and the tweets filtered for only those containing "delayed". These tweets were then groups by airline, and their frequency plotted, shown below. We see that United and US Airways have the worst reputation for tardiness.

```{r, echo=FALSE}
review$word[grepl('delays', review$word)] <- 'delayed'
review$word[grepl('delay', review$word)] <- 'delayed'

#next, we plot the data for delayed complaints by airline

delayed <- review %>%
  filter(word == 'delayed') %>%
  select(airline) %>%
  group_by(airline) %>%
  summarise(freq = n())

ggplot(delayed, aes(x = airline, y = freq)) +
  geom_bar(stat = 'identity', fill='blue') +
  theme_set(theme_bw()) +
  theme(axis.title.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Delayed Complaints by Airline", x="Airline", y="Number of Complaints")

```


### Positive Sentiments

Despite not having a column categorizing the reasons for positive sentiments, we also created a word cloud to show the most frequently occuring word in the positive sentiments. The results, shown below, reveal that the second most common reason for positive sentiments are in regards to refunds, which can only have been associated with an initial negaative experience.

```{r, echo=FALSE}

###counting positive sentiments
positive <- get_sentiments('bing') %>%
  filter(sentiment == "positive")

pos_reviews <- review %>%
  inner_join(positive) %>%
  count(word, sort=TRUE)

#head(pos_reviews, 10)

pos_reviews <- review %>%
  inner_join(positive) %>%
  count(word) %>%
  with(wordcloud(word, n,
                 rot.per = .15,
                 scale = c(5, .3),
                 max.words = 50,
                 random.order = F,
                 random.color = F,
                 colors = pal))

```

As was done with the "delayed", we wanted to see which airlines are most associated with the refunds. To do this, all words rooted with "refund" were converted to "refund", The tweets then filtered for those associated with refunds, grouped by airline and plotted below. Once again, United and US airlines, which are associated with most negative sentiments ultimately are also issuing the most refunds. 

```{r, echo=FALSE}
#we can see 'refund' is a pretty high pos review, let's check how many people were refunded by airline
#sum(review$word=='refunded')#refunded shows up 19 times. Let's change that to refund

review$word[grepl('refunded', review$word)] <- 'refund'

refund <- review %>%
  filter(word == 'refund') %>%
  select(airline) %>%
  group_by(airline) %>%
  summarise(freq = n())

ggplot(refund, aes(x = airline, y = freq)) +
  geom_bar(stat = 'identity', fill='red') +
  theme_set(theme_bw()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Refunded Tickets by Airline", x="Airline", y="Number of Recorded Refunds")

```


### Retweet Revelations

Counting the number of retweets can help to uncover more information about customer sentiments. May people do not comment on their experience, good or bad, but may express their sentiments by retweeting comments that reflect them. Below, we plotted retweet counts with respect to airline, negative reasons, and positive sentments. It can be suggested from the graph below that negative sentiments for United and US Airways are shared greatly in the wider Twitter community, several times that of the neutral and positive sentiments combined. This negativity is shared to a lesser extent by customers of American Airlines. While Delta has high number of negative sentiment retweets, the wider community overall appear to share neutral or positive sentiments. South Western and Virgin American on the other hand, appear to have a much more positive image amongst the wider community.

```{r, echo=FALSE}
###  Looking at retweet counts and airline
retweet_1 <- df %>%
  select(airline, airline_sentiment, retweet_count) %>%
  group_by(airline, airline_sentiment) %>% 
  summarise(total = sum(retweet_count))

ggplot(retweet_1, aes(x = airline, y = total, fill = airline_sentiment)) + 
  geom_bar(stat="identity", position = position_dodge()) + 
  theme_set(theme_bw()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Total number of Retweets by Airline", x = 'Airline', y = 'Number of Retweets')
```


Taking a deeper look at the retweets within the negative sentiments, we can see where the wider Twitter community share their negative experiences. As previously seen, most negative tweets were related to customer service, and the graph below also reflects similar senitments, along with tardiness. It can be concluded that United has a poor reputation across all negative categories, which the other airlines can narrow down specific areas for improvement.

```{r, echo=FALSE}
###  Looking at retweet counts and negative reasons
retweet_2 <- df %>%
  filter(airline_sentiment == "negative") %>%
  select(airline, negativereason, retweet_count) %>%
  group_by(airline, negativereason) %>%
  summarise(total = sum(retweet_count))

ggplot(retweet_2, aes(x = negativereason, y = total)) + 
  geom_bar(stat="identity", fill="blue") + 
  theme_set(theme_bw()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title="Total number of Retweets by Negative Reason", x = 'Negative Reasons', y = 'Number of Retweets') +
  facet_wrap(~ airline)

```


Exploring the number of retweets for positive sentiments can also expose which airlines resonate most positively with the wider Twitterverse. The graph below shows that the wider community share most positive sentiments with Delta and Southwest. When compared with number of negative retweets, this might suggest that these airlines do a far better job in meeting customer expectations when addressing issues.  

```{r, echo=FALSE}
#retweet count for positive reasons

retweet_3 <- df %>%
  filter(airline_sentiment == "positive") %>%
  select(airline, retweet_count) %>%
  group_by(airline) %>%
  summarise(total = sum(retweet_count))


ggplot(retweet_3, aes(x = airline, y= total)) +
  geom_bar(stat= "identity", fill = "blue") +
  theme_set(theme_bw()) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Total Number of Retweets by Positive Sentiment", x = "Airline", y = "Number of Retweets")

```


Proceeding with our own text analysis of the tweets in question, we expect to obtain a similar breakdown of sentiments for the negatives, but also expect that we will be able to determine with more accuracy what the airlines are doing correctly.

********Alter this as you get a clearer idea from write up.



# Model Development and Validation

```{r, echo=FALSE}
data <- tidytext::unnest_tokens(read.csv('Tweets.csv', na.strings = c("", "NA"), stringsAsFactors = FALSE), word, text)


data <- subset(data, select = c(tweet_id, word))#leaves only our terms and IDs
```


To contruct our model, we first tokenized all the tweets based on words, and extracted "tweet_id" column along with the (tokenized) "words" column. We then cleaned our list of tokenized words by removing digits, punctuation, stop words, and any remaining blank spaces. The data was then indexed and grouped by tweet_id. For each unique id, the individual keywords were spread across the dataframe by their value of importance. Some tweets hold only two or three keywords in a row, while others contain several more. Topic modelling using LDA requires the document to contain the key text in one row per id, after which the model will conditionally assign topics and extract terms which are correlated to those topics. Our tokenized words were then united, with all punctuation and digits removed in preparation for the model to extract key insights.

```{r, echo=TRUE}
clean_tokens <- data
clean_tokens$word <- gsubfn('[[:digit:]]+', '', clean_tokens$word)
clean_tokens$word <- gsubfn('[[:punct:]]+', '', clean_tokens$word)
data("stop_words")

clean_tokens <- clean_tokens %>%
  filter(!(nchar(word) == 1))%>% 
  anti_join(stop_words)

data_tokens <- clean_tokens %>%
  filter(!(word==""))

data_tokens <- data_tokens %>%
  mutate(ind = row_number())

data_tokens <- data_tokens %>%
  group_by(tweet_id) %>%
  mutate(ind = row_number()) %>%
  tidyr::spread(key = ind, value = word)

data_tokens[is.na(data_tokens)] <- ""

data_tokens <- tidyr::unite(data_tokens, word,-tweet_id,sep =" " )
data_tokens$word <- trimws(data_tokens$word)

```

Using the now clean tokenized words, a Document Term Matrix of dgCMatrix-class was created, naming the documents based on tweet_id and using an ngram window up to 2. Upon checking the frequency of the 83209 individual terms, terms that occurred only once or appeared in more than half of all the documents were removed, as shown below. 

```{r, echo=TRUE}
#Document Term Matrix

dtm <- CreateDtm(data_tokens$word,
                 doc_names = data_tokens$tweet_id,
                 ngram_window = c(1, 2),
                 stopword_vec = c(stopwords::stopwords(language = "en",
                                                       source = "smart")))

#Term Frequency

tf <- TermDocFreq(dtm = dtm)

original_tf <- tf %>%
  select(term, term_freq, doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)

#remove any words that appear <2 or in >half the doc

vocab <- tf$term[tf$term_freq >1 & tf$doc_freq < nrow(dtm)/2]

dtm = dtm

```


### Latent Dirichlet Allocation (LDA) Model using the LDA Function

In order to find meaning within the tweets, we applied the Latent Dirichlet Allocation (LDA) model to determine 9 possible topics within each tweet document (k = 9). In the first run we used the LDA Function, which employs the VEM algorithm. This created a LDA_VEM topic model with 9 topics. The topic probabilities were extracted from the LDA into matrix format and organized in descending order of probability. The top 10 terms per topic are plotted below, where "beta" is the probablity. 

```{r, echo=TRUE}
#use Latent Dirichlet Allocation

data_lda <- LDA(dtm, k=9, control = list(seed = 1234))
#data_lda

#check per-topic per-word probabilites
data_topics <- tidy(data_lda, matrix = 'beta')
#data_topics

data_top_terms <- data_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

```
```{r, echo = FALSE}

data_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

```


### Latent Dirichlet Allocation (LDA) Model using the FitLdaModel Function

For the second run, the FitLdaModel Function was used to build the model. Both methods work well, but have different strengths: the previous method, which employs the VEM algorithm, requires less work and doesn't have as many calculations, while the method FitLdaModel function, which employs Gibb's Sampling, allows for the creation of topic labels easily using theta values. In the model below, alpha and beta control the distribution of topics and terms, where a high alpha and beta correspond to more terms and topics. To limit the number of topics and terms we use low values.

```{r, echo=TRUE}

model <- FitLdaModel(dtm = dtm, k = 9, iterations = 200,
                     burnin = 180, alpha = 0.1, beta = 0.05,
                     optimize_alpha = TRUE, calc_likelihood = TRUE,
                     calc_coherence = TRUE, calc_r2 = TRUE, cpus = 2)
```

The model calculated R-squared at 0.02827366 signifying a low proportion of variability in terms. This suggests that the terms typically revolve around the same topics, which in the case of airline services is expected. We plot the log likelihood with respect to the number of iterations below.
```{r, echo=FALSE}
model$r2

plot(model$log_likelihood, type='l')
#we probably don't need the log_likelihood and r-squared numbers
#not working a regression model so it's not truly necessary
```

Below is a statistics summary and histogram of the term coherence.

```{r, echo=FALSE}
summary(model$coherence)#check term coherence (don't expect high numbers)
hist(model$coherence, col = 'blue', main = "Histogram of probabilistic coherence")
```

From the model, we then extracted the top 10 terms per topic and plotted the prevelence of each topic.
```{r, echo=FALSE}
#The top 10 terms per topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 10)
head(t(model$top_terms))

#The prevalence of each topic
model$prevalence <- colSums(model$theta)/sum(model$theta) * 100

#plot prevalence; should be proportional to alpha
plot(model$prevalence, model$alpha, xlab = 'prevalence', ylab = 'alpha')

```

From here we labeled our model topics using theta values greater than 0.05, and combined the topics, coherence score, prevelance and top terms into one dataframe, displayed below in html format. 

```{r, echo=FALSE}

model$labels <- LabelTopics(assignments = model$theta > 0.05,
                            dtm = dtm,
                            M = 1)

#head(model$labels)

#Combine together; topics, coherence score, prevalence, top terms
model$summary <- data.frame(topic = rownames(model$phi),
                            label = model$labels,
                            coherence = round(model$coherence, 3),
                            prevalence = round(model$prevalence, 4),
                            top_terms = apply(model$top_terms, 2,
                                              function(x){
                                                paste(x, collapse = ", ")
                                              }),
                            stringsAsFactors = FALSE)

#create a table showing LDA-intuited topic labels
#htmlTable(model$summary[ order(model$summary$prevalence,
#                               decreasing = TRUE), ][1:10, ])

as_tibble(model$summary)
```


### Model Validation

Once both models were created, we compared the two methods ability to accurately predict topics. Noting that Gibb's method uses conditional probability to decide topics, while the dot method calculates dot-product probability of topics, the plot below shows which method works best per topic. Overall, while each method have their own pros and cons, and Gibb's method is slower, we ultimately chose the Gibb's prediction model.

```{r, echo=FALSE}
#Our model was fitted using the Gibbs method for LDA

#Gibb's Method
assignments <- predict(model, dtm,
                       method = "gibbs",
                       iterations = 200,
                       burnin = 180,
                       cpus = 2)

#Dot method
assignments_dot <- predict(model, dtm,
                           method = "dot")

#Compare the two methods
barplot(rbind(assignments[10,], assignments_dot[10,]),
        col = c("red", "blue"), las = 2, beside = TRUE)
legend("topright", legend = c("gibbs", "dot"), col = c("red", "blue"), 
       fill = c("red", "blue"))
#plot shows per topic which method works best. Overall, gibbs is slower
#but there are benefits to each


```


# Implementation in Shiny 

```{r, echo=FALSE}

```


# Limitations

Twitter data inherently comes with its own limitations. While text mining social media allows companies to gain a better understanding of client sentiments, Twitter’s limiting character allowance leaves little room for much to be gained beyond the client creating a thread. Even in those instances, threads are more likely to contain negative sentiments regarding customer experience. As the tweets we mined were from the era of Twitter’s 140 character limit, it is possible that more recent data would allow companies to gain better insights. Of course, with a character limit, and with the focus of Twitter being a platform to allow customers to express their thoughts in limited words, the insights gained tend to be either positive or negative, with little room for nuance. In such a limited setting, customers would more likely use the platform to air their complaints, leading to an increasingly higher number of negative reviews from such platforms. We cannot discredit Twitter data because of the limited characters; when we see words revolving around customer service such as in topics 4 and 5, we can see key terms that could show an area airlines need improvement on, or they could find it’s an area they are doing well in.

In our dataset, we had 15 variables; tweet_id, airline_sentiment, airline_sentiment_confidence, negativereason, negativereason_confidence, airline, airline_sentiment_gold, name, negativereason_gold, retweet_count, text, tweet_coord, tweet_created, tweet_location, user_timezone. Twitter users are allowed, for privacy concerns and other reasons, to decide whether Twitter is allowed to use their location or coordinates. In addition, Twitter allows them to specify their locations in their own words. A user could input their location as “Somewhere over the rainbow”, for example. With this option available to users, companies seeking to understand whether there are trends in airline sentiment based on location will find it very difficult to do so. In cases such as those, perhaps airline reviews on other sites would provide better insights.



# Conclusion: Insights/How Insights can be used to improve business

Our goal for this project was to conduct text analysis using natural language processing on a series of tweets pertaining to six major US airlines, that could be used to generate insights that these companies could potential use to improve the quality of their services. We tried two methods to build a Latent Dirichlet Allocation (LDA) model, used to predict possilbe topics within each tweet document. Our secondary model, using the textmineR FitLdaModel function, allowed us to get a closer look at topic labels and pull together a table with labelled topics and terms. 

*************Discuss the insights that we can get from this, and how it related to data exploration.
*************Discuss how shiny app helps to achieve that goal. 
*************Link to shiny app
